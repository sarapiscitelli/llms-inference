HOSTNAME=0.0.0.0
PORT=8080

DOWNLOAD_MODELS_DIR="" # the local directory where the models are located
MODEL_NAME=mistral-7b-instruct-v0.1.Q4_0.gguf
N_GPU_LAYERS=-1 # The number of layers to put on the GPU. The rest will be on the CPU. Set -1 to move all to GPU
N_CTX=1024 # The allowed context length for the user in tokens (is not the model maximum length)


# For more option look at: https://github.com/abetlen/llama-cpp-python/blob/dccbac82ebad865a9332bff248b871417cd439a1/llama_cpp/server/app.py#L38

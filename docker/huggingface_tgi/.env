HOSTNAME=0.0.0.0
PORT=8080

MODEL_ID=google/flan-t5-large # mistralai/Mistral-7B-Instruct-v0.1
MAX_INPUT_LENGTH=2048 #  maximum allowed input length (expressed in number of tokens) for users (is not for the model!).
MAX_TOTAL_TOKENS=4096 # MAX_INPUT_LENGTH < MAX_TOTAL_TOKENS 
DTYPE=float16 # optional, possible values: float16, bfloat16 The dtype to be forced upon the model. This option cannot be used with `QUANTIZE`
QUANTIZE= # optional, quantize the model ([possible values: awq, eetq, gptq, bitsandbytes, bitsandbytes-nf4, bitsandbytes-fp4]). Not set together with `DTYPE`
HUGGING_FACE_HUB_TOKEN= # optional (valid only for private models)
CUDA_VISIBLE_DEVICES=# 0,1

# look at other options here: https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs

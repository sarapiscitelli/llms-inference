HOSTNAME=0.0.0.0
PORT=8080

MODEL="/mistral-7b-instruct-v0.1.Q8_0.gguf"  # full path to a downloaded model (.gguf format)
N_GPU_LAYERS=-1 # The number of layers to put on the GPU. The rest will be on the CPU. Set -1 to move all to GPU
# N_CTX=2048 # The allowed context length for the user in tokens (is not the model maximum length)

# For more option look at: https://github.com/abetlen/llama-cpp-python/blob/dccbac82ebad865a9332bff248b871417cd439a1/llama_cpp/server/app.py#L38
